{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. PSPNet_훈련.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNMkNhh6FNUZF2Hn1tGj7mF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vySn5Wu1kaIu","executionInfo":{"status":"ok","timestamp":1644932745434,"user_tz":-540,"elapsed":16886,"user":{"displayName":"이준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14894232288140235299"}},"outputId":"307a5d8d-2da9-4898-9888-12af553e98b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["mfrom google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# 학습 및 검증 실시\n"," - PSPNet 학습 및 검증을 실시.\n"," - p2.xlarge에서는 약 12시간이 걸림"],"metadata":{"id":"y2UxcpEKkdji"}},{"cell_type":"markdown","source":["# 학습 목표\n","  1. PSPNet 학습 및 검증을 구현할 수 있다\n","  2. 시맨틱 분할의 파인 튜닝을 이해한다\n","  3. 스케줄러로 epoch마다 학습률을 변화시키는 기법을 구현할 수 있다"],"metadata":{"id":"izWn0AeTlAZK"}},{"cell_type":"code","source":["# 패키지 import\n","import random\n","import math\n","import time\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.utils.data as data\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","import torch.optim as optim"],"metadata":{"id":"W9ZoiuUelJ86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 초기설정\n","# Setup seeds\n","torch.manual_seed(1234)\n","np.random.seed(1234)\n","random.seed(1234)"],"metadata":{"id":"sstKOKO6lNy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Colab Notebooks/만들면서 배우는 파이토치 딥러닝/3. 시맨틱 분할(PSPNet)'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQnZamXSlO6M","executionInfo":{"status":"ok","timestamp":1644932751127,"user_tz":-540,"elapsed":415,"user":{"displayName":"이준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14894232288140235299"}},"outputId":"11ff15fc-2f68-4a20-c136-dde1b0033e63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/만들면서 배우는 파이토치 딥러닝/3. 시맨틱 분할(PSPNet)\n"]}]},{"cell_type":"markdown","source":["# DataLoader 작성"],"metadata":{"id":"IYWzCpOcldW7"}},{"cell_type":"code","source":["from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n","\n","# 파일 경로 리스트 작성\n","rootpath = \"/content/drive/MyDrive/Colab Notebooks/만들면서 배우는 파이토치 딥러닝/3. 시맨틱 분할(PSPNet)/data/VOCdevkit/VOC2012/\"\n","train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n","    rootpath = rootpath\n",")\n","\n","# Dataset 작성\n","# (RGB) 색의 평균값과 표준편차\n","color_mean = (0.485, 0.456, 0.406)\n","color_std = (0.229, 0.224, 0.225)\n","\n","train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=DataTransform(\n","    input_size=475, color_mean = color_mean, color_std = color_std))\n","\n","val_dataset = VOCDataset(val_img_list, val_anno_list, phase = \"val\", transform=DataTransform(\n","    input_size = 475, color_mean = color_mean, color_std = color_std))\n","\n","# DataLoader 작성\n","batch_size = 8\n","\n","train_dataloader = data.DataLoader(\n","    train_dataset, batch_size = batch_size, shuffle = True\n",")\n","\n","val_dataloader = data.DataLoader(\n","  val_dataset, batch_size=batch_size, shuffle = False\n",")\n","\n","# 사전형 변수로 정리\n","dataloaders_dict = {\"train\" : train_dataloader, \"val\" : val_dataloader}\n"],"metadata":{"id":"aFQgTZKnlfhb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 네트워크 모델 작성"],"metadata":{"id":"kURDEJQWme75"}},{"cell_type":"code","source":["from utils.pspnet import PSPNet\n","\n","# 파인 튜닝으로 PSPNet을 작성\n","# ADE20K 데이터 세트의 학습된 모델을 사용하며, ADE20K는 클래스 수가 150\n","net = PSPNet(n_classes = 150)\n","\n","# ADE20K 학습된 파라미터를 읽기\n","state_dict = torch.load(\"/content/drive/MyDrive/Colab Notebooks/만들면서 배우는 파이토치 딥러닝/3. 시맨틱 분할(PSPNet)/weights/pspnet50_ADE20K.pth\")\n","net.load_state_dict(state_dict)\n","\n","# 분류용의 합성곱층을, 출력수 21으로 바꿈\n","n_classes = 21\n","net.decode_feature.classification = nn.Conv2d(\n","    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n","\n","net.aux.classification = nn.Conv2d(\n","    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n","\n","# 교체한 합성곱층을 초기화한다. 활성화 함수는 시그모이드 함수이므로 Xavier를 사용한다.\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        nn.init.xavier_normal_(m.weight.data)\n","        if m.bias is not None:  # 바이어스 항이 있는 경우\n","            nn.init.constant_(m.bias, 0.0)\n","\n","net.decode_feature.classification.apply(weights_init)\n","net.aux.classification.apply(weights_init)\n","\n","print('네트워크 설정 완료: 학습된 가중치를 로드했습니다')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uY1VOZyRmoax","executionInfo":{"status":"ok","timestamp":1644932759491,"user_tz":-540,"elapsed":3527,"user":{"displayName":"이준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14894232288140235299"}},"outputId":"5c9041a3-2ee8-40cf-8128-fc89bdf3994b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["네트워크 설정 완료: 학습된 가중치를 로드했습니다\n"]}]},{"cell_type":"code","source":["net"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCz0YWvtoRX8","executionInfo":{"status":"ok","timestamp":1644932759493,"user_tz":-540,"elapsed":19,"user":{"displayName":"이준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14894232288140235299"}},"outputId":"e13321cc-07ee-4aab-dead-cf378fde364a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PSPNet(\n","  (feature_conv): FeatureMap_convolution(\n","    (cbnr_1): conv2DBatchNormRelu(\n","      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (cbnr_2): conv2DBatchNormRelu(\n","      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (cbnr_3): conv2DBatchNormRelu(\n","      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  )\n","  (feature_res_1): ResidualBlockPSP(\n","    (block1): bottleNeckPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (cb_residual): conv2DBatchNorm(\n","        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block2): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block3): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (feature_res_2): ResidualBlockPSP(\n","    (block1): bottleNeckPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (cb_residual): conv2DBatchNorm(\n","        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block2): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block3): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block4): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (feature_dilated_res_1): ResidualBlockPSP(\n","    (block1): bottleNeckPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (cb_residual): conv2DBatchNorm(\n","        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block2): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block3): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block4): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block5): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block6): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (feature_dilated_res_2): ResidualBlockPSP(\n","    (block1): bottleNeckPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (cb_residual): conv2DBatchNorm(\n","        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block2): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","    (block3): bottleNeckIdentifyPSP(\n","      (cbr_1): conv2DBatchNormRelu(\n","        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cbr_2): conv2DBatchNormRelu(\n","        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n","        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (cb_3): conv2DBatchNorm(\n","        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (pyramid_pooling): PyramidPooling(\n","    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n","    (cbr_1): conv2DBatchNormRelu(\n","      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n","    (cbr_2): conv2DBatchNormRelu(\n","      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n","    (cbr_3): conv2DBatchNormRelu(\n","      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n","    (cbr_4): conv2DBatchNormRelu(\n","      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (decode_feature): DecodePSPFeature(\n","    (cbr): conv2DBatchNormRelu(\n","      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (dropout): Dropout2d(p=0.1, inplace=False)\n","    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (aux): AuxiliaryPSPlayers(\n","    (cbr): conv2DBatchNormRelu(\n","      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (dropout): Dropout2d(p=0.1, inplace=False)\n","    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["# 손실함수 정의"],"metadata":{"id":"TUPw9xPAoa7b"}},{"cell_type":"code","source":["# 손실함수 정의\n","class PSPLoss(nn.Module):\n","    \"\"\"PSPNet의 손실함수 클래스입니다\"\"\"\n","\n","    def __init__(self, aux_weight=0.4):\n","        super(PSPLoss, self).__init__()\n","        self.aux_weight = aux_weight  # aux_loss의 가중치\n","\n","    def forward(self, outputs, targets):\n","        \"\"\"\n","        손실함수 계산\n","\n","        Parameters\n","        ----------\n","        outputs : PSPNet의 출력(tuple)\n","            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。\n","\n","        targets : [num_batch, 475, 475]\n","            정답 어노테이션 정보\n","\n","        Returns\n","        -------\n","        loss : 텐서\n","            손실값\n","        \"\"\"\n","\n","        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n","        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n","\n","        return loss+self.aux_weight*loss_aux\n","\n","\n","criterion = PSPLoss(aux_weight=0.4)\n"],"metadata":{"id":"O4v2WCpvodsG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 최적화 기법 설정"],"metadata":{"id":"m4cL4xTtohTl"}},{"cell_type":"code","source":["# 파인 튜닝이므로, 학습률은 작게\n","optimizer = optim.SGD([\n","    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n","    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n","    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n","    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n","    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n","    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n","    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n","    {'params': net.aux.parameters(), 'lr': 1e-2},\n","], momentum=0.9, weight_decay=0.0001)\n","\n","\n","# 스케쥴러 설정\n","def lambda_epoch(epoch):\n","    max_epoch = 30\n","    return math.pow((1-epoch/max_epoch), 0.9)\n","\n","\n","scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n"],"metadata":{"id":"ZDMeEMqlojBj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 학습 및 검증 실시"],"metadata":{"id":"MYGBs7tVokUA"}},{"cell_type":"code","source":["# 모델을 학습시키는 함수를 작성\n","def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n","\n","    # GPU가 사용 가능한지 확인\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"사용 장치: \", device)\n","\n","    # 네트워크를 GPU로\n","    net.to(device)\n","\n","    # 네트워크가 어느 정도 고정되면 고속화한다\n","    torch.backends.cudnn.benchmark = True\n","\n","    # 화상의 매수\n","    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n","    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n","    batch_size = dataloaders_dict[\"train\"].batch_size\n","\n","    # 반복자의 카운터 설정\n","    iteration = 1\n","    logs = []\n","\n","    # multiple minibatch\n","    batch_multiplier = 3\n","\n","    # epoch 루프\n","    for epoch in range(num_epochs):\n","\n","        # 시작 시간 저장\n","        t_epoch_start = time.time()\n","        t_iter_start = time.time()\n","        epoch_train_loss = 0.0  # epoch의 손실합\n","        epoch_val_loss = 0.0  # epoch의 손실합\n","\n","        print('-------------')\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-------------')\n","\n","        # epoch별 훈련 및 검증 루프\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train()  # 모델을 훈련 모드로\n","                scheduler.step()  # 최적화 scheduler 갱신\n","                optimizer.zero_grad()\n","                print('(train)')\n","\n","            else:\n","                if((epoch+1) % 5 == 0):\n","                    net.eval()   # 모델을 검증 모드로\n","                    print('-------------')\n","                    print('(val)')\n","                else:\n","                    # 검증은 다섯 번 중에 한 번만 수행\n","                    continue\n","\n","            # 데이터 로더에서 minibatch씩 꺼내 루프\n","            count = 0  # multiple minibatch\n","            for imges, anno_class_imges in dataloaders_dict[phase]:\n","                # 미니배치 크기가 1이면 배치 노멀라이제이션에서 오류가 발생하므로 회피\n","                if imges.size()[0] == 1:\n","                    continue\n","\n","                # GPU가 사용가능하면 GPU에 데이터를 보낸다\n","                imges = imges.to(device)\n","                anno_class_imges = anno_class_imges.to(device)\n","\n","                # multiple minibatch로 파라미터 갱신\n","                if (phase == 'train') and (count == 0):\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    count = batch_multiplier\n","\n","                # 순전파(forward) 계산\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = net(imges)\n","                    loss = criterion(\n","                        outputs, anno_class_imges.long()) / batch_multiplier\n","\n","                    # 훈련시에는 역전파\n","                    if phase == 'train':\n","                        loss.backward()  # 경사 계산\n","                        count -= 1  # multiple minibatch\n","\n","                        if (iteration % 10 == 0):  # 10iter에 한 번, loss를 표시\n","                            t_iter_finish = time.time()\n","                            duration = t_iter_finish - t_iter_start\n","                            print('반복 {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n","                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n","                            t_iter_start = time.time()\n","\n","                        epoch_train_loss += loss.item() * batch_multiplier\n","                        iteration += 1\n","\n","                    # 검증 시\n","                    else:\n","                        epoch_val_loss += loss.item() * batch_multiplier\n","\n","        # epoch의 phase별 loss와 정답률\n","        t_epoch_finish = time.time()\n","        print('-------------')\n","        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n","            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n","        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n","        t_epoch_start = time.time()\n","\n","        # 로그 저장\n","        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n","                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n","        logs.append(log_epoch)\n","        df = pd.DataFrame(logs)\n","        df.to_csv(\"log_output.csv\")\n","\n","    # 최후의 네트워크를 저장\n","    torch.save(net.state_dict(), '/content/drive/MyDrive/Colab Notebooks/만들면서 배우는 파이토치 딥러닝/3. 시맨틱 분할(PSPNet)/weights/' +\n","               str(epoch+1) + '.pth')\n"],"metadata":{"id":"PUOTyNaDol43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 및 검증 실행\n","num_epochs = 30\n","train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sRhJHtIonEi","executionInfo":{"status":"ok","timestamp":1644943974064,"user_tz":-540,"elapsed":11124762,"user":{"displayName":"이준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14894232288140235299"}},"outputId":"0de9ae2a-992c-4411-8296-df5b47c2d873"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["사용 장치:  cuda:0\n","-------------\n","Epoch 1/30\n","-------------\n","(train)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["반복 10 || Loss: 0.4176 || 10iter: 214.3335 sec.\n","반복 20 || Loss: 0.4931 || 10iter: 84.9883 sec.\n","반복 30 || Loss: 0.1544 || 10iter: 85.0755 sec.\n","반복 40 || Loss: 0.2496 || 10iter: 84.2611 sec.\n","반복 50 || Loss: 0.1149 || 10iter: 84.0570 sec.\n","반복 60 || Loss: 0.1224 || 10iter: 82.8153 sec.\n","반복 70 || Loss: 0.1707 || 10iter: 84.6865 sec.\n","반복 80 || Loss: 0.1239 || 10iter: 85.6735 sec.\n","반복 90 || Loss: 0.0910 || 10iter: 104.7512 sec.\n","반복 100 || Loss: 0.1568 || 10iter: 84.6982 sec.\n","반복 110 || Loss: 0.2090 || 10iter: 83.1227 sec.\n","반복 120 || Loss: 0.1453 || 10iter: 85.0443 sec.\n","반복 130 || Loss: 0.0764 || 10iter: 83.4556 sec.\n","반복 140 || Loss: 0.0696 || 10iter: 85.4614 sec.\n","반복 150 || Loss: 0.1357 || 10iter: 83.7235 sec.\n","반복 160 || Loss: 0.1312 || 10iter: 84.5904 sec.\n","반복 170 || Loss: 0.1508 || 10iter: 85.1623 sec.\n","반복 180 || Loss: 0.2796 || 10iter: 84.1337 sec.\n","-------------\n","epoch 1 || Epoch_TRAIN_Loss:0.1790 ||Epoch_VAL_Loss:0.0000\n","timer:  1715.4353 sec.\n","-------------\n","Epoch 2/30\n","-------------\n","(train)\n","반복 190 || Loss: 0.0657 || 10iter: 8.8564 sec.\n","반복 200 || Loss: 0.0628 || 10iter: 13.0937 sec.\n","반복 210 || Loss: 0.0870 || 10iter: 13.1229 sec.\n","반복 220 || Loss: 0.1087 || 10iter: 13.0932 sec.\n","반복 230 || Loss: 0.0809 || 10iter: 13.0855 sec.\n","반복 240 || Loss: 0.0649 || 10iter: 13.0856 sec.\n","반복 250 || Loss: 0.0927 || 10iter: 13.1254 sec.\n","반복 260 || Loss: 0.0496 || 10iter: 13.0638 sec.\n","반복 270 || Loss: 0.0913 || 10iter: 13.1742 sec.\n","반복 280 || Loss: 0.0975 || 10iter: 13.1268 sec.\n","반복 290 || Loss: 0.0618 || 10iter: 13.1024 sec.\n","반복 300 || Loss: 0.0369 || 10iter: 13.1473 sec.\n","반복 310 || Loss: 0.1379 || 10iter: 13.1898 sec.\n","반복 320 || Loss: 0.0704 || 10iter: 13.1061 sec.\n","반복 330 || Loss: 0.1411 || 10iter: 13.1042 sec.\n","반복 340 || Loss: 0.0700 || 10iter: 13.1282 sec.\n","반복 350 || Loss: 0.0783 || 10iter: 13.1844 sec.\n","반복 360 || Loss: 0.0692 || 10iter: 13.0776 sec.\n","-------------\n","epoch 2 || Epoch_TRAIN_Loss:0.0927 ||Epoch_VAL_Loss:0.0000\n","timer:  260.9821 sec.\n","-------------\n","Epoch 3/30\n","-------------\n","(train)\n","반복 370 || Loss: 0.0431 || 10iter: 4.5719 sec.\n","반복 380 || Loss: 0.0513 || 10iter: 13.0794 sec.\n","반복 390 || Loss: 0.1028 || 10iter: 13.1178 sec.\n","반복 400 || Loss: 0.0639 || 10iter: 13.1220 sec.\n","반복 410 || Loss: 0.0730 || 10iter: 13.1200 sec.\n","반복 420 || Loss: 0.0735 || 10iter: 13.1589 sec.\n","반복 430 || Loss: 0.0901 || 10iter: 13.1875 sec.\n","반복 440 || Loss: 0.0527 || 10iter: 13.1107 sec.\n","반복 450 || Loss: 0.1021 || 10iter: 13.1689 sec.\n","반복 460 || Loss: 0.0478 || 10iter: 13.1066 sec.\n","반복 470 || Loss: 0.0535 || 10iter: 13.1353 sec.\n","반복 480 || Loss: 0.0752 || 10iter: 13.0879 sec.\n","반복 490 || Loss: 0.0499 || 10iter: 13.0758 sec.\n","반복 500 || Loss: 0.0780 || 10iter: 13.1263 sec.\n","반복 510 || Loss: 0.0956 || 10iter: 13.1609 sec.\n","반복 520 || Loss: 0.1862 || 10iter: 13.0974 sec.\n","반복 530 || Loss: 0.0592 || 10iter: 13.1622 sec.\n","반복 540 || Loss: 0.0402 || 10iter: 13.1011 sec.\n","-------------\n","epoch 3 || Epoch_TRAIN_Loss:0.0811 ||Epoch_VAL_Loss:0.0000\n","timer:  261.1217 sec.\n","-------------\n","Epoch 4/30\n","-------------\n","(train)\n","반복 550 || Loss: 0.0956 || 10iter: 0.2829 sec.\n","반복 560 || Loss: 0.0814 || 10iter: 13.1206 sec.\n","반복 570 || Loss: 0.0556 || 10iter: 13.1280 sec.\n","반복 580 || Loss: 0.0664 || 10iter: 13.1622 sec.\n","반복 590 || Loss: 0.1693 || 10iter: 13.1466 sec.\n","반복 600 || Loss: 0.0454 || 10iter: 13.1162 sec.\n","반복 610 || Loss: 0.0652 || 10iter: 13.1288 sec.\n","반복 620 || Loss: 0.0527 || 10iter: 13.1591 sec.\n","반복 630 || Loss: 0.0901 || 10iter: 13.1405 sec.\n","반복 640 || Loss: 0.0469 || 10iter: 13.1162 sec.\n","반복 650 || Loss: 0.0810 || 10iter: 13.1032 sec.\n","반복 660 || Loss: 0.0636 || 10iter: 13.0875 sec.\n","반복 670 || Loss: 0.0845 || 10iter: 13.0828 sec.\n","반복 680 || Loss: 0.1009 || 10iter: 13.1692 sec.\n","반복 690 || Loss: 0.0757 || 10iter: 13.1389 sec.\n","반복 700 || Loss: 0.1594 || 10iter: 13.1534 sec.\n","반복 710 || Loss: 0.0446 || 10iter: 13.1252 sec.\n","반복 720 || Loss: 0.0302 || 10iter: 13.0961 sec.\n","반복 730 || Loss: 0.0777 || 10iter: 13.1407 sec.\n","-------------\n","epoch 4 || Epoch_TRAIN_Loss:0.0715 ||Epoch_VAL_Loss:0.0000\n","timer:  261.1670 sec.\n","-------------\n","Epoch 5/30\n","-------------\n","(train)\n","반복 740 || Loss: 0.0550 || 10iter: 10.2854 sec.\n","반복 750 || Loss: 0.0674 || 10iter: 13.1087 sec.\n","반복 760 || Loss: 0.1122 || 10iter: 13.1548 sec.\n","반복 770 || Loss: 0.0465 || 10iter: 13.1794 sec.\n","반복 780 || Loss: 0.0452 || 10iter: 13.1065 sec.\n","반복 790 || Loss: 0.0454 || 10iter: 13.1161 sec.\n","반복 800 || Loss: 0.0839 || 10iter: 13.0627 sec.\n","반복 810 || Loss: 0.0525 || 10iter: 13.1510 sec.\n","반복 820 || Loss: 0.1005 || 10iter: 13.1149 sec.\n","반복 830 || Loss: 0.0203 || 10iter: 13.1533 sec.\n","반복 840 || Loss: 0.0680 || 10iter: 13.1499 sec.\n","반복 850 || Loss: 0.0514 || 10iter: 13.1465 sec.\n","반복 860 || Loss: 0.0666 || 10iter: 13.1136 sec.\n","반복 870 || Loss: 0.0407 || 10iter: 13.1819 sec.\n","반복 880 || Loss: 0.0612 || 10iter: 13.1649 sec.\n","반복 890 || Loss: 0.0485 || 10iter: 13.0804 sec.\n","반복 900 || Loss: 0.0532 || 10iter: 13.1220 sec.\n","반복 910 || Loss: 0.0417 || 10iter: 13.0748 sec.\n","-------------\n","(val)\n","-------------\n","epoch 5 || Epoch_TRAIN_Loss:0.0633 ||Epoch_VAL_Loss:0.0811\n","timer:  1639.1954 sec.\n","-------------\n","Epoch 6/30\n","-------------\n","(train)\n","반복 920 || Loss: 0.0544 || 10iter: 5.9894 sec.\n","반복 930 || Loss: 0.0453 || 10iter: 13.1404 sec.\n","반복 940 || Loss: 0.2249 || 10iter: 13.1314 sec.\n","반복 950 || Loss: 0.0511 || 10iter: 13.0907 sec.\n","반복 960 || Loss: 0.0754 || 10iter: 13.0903 sec.\n","반복 970 || Loss: 0.0919 || 10iter: 13.1432 sec.\n","반복 980 || Loss: 0.0694 || 10iter: 13.1231 sec.\n","반복 990 || Loss: 0.0434 || 10iter: 13.1502 sec.\n","반복 1000 || Loss: 0.0628 || 10iter: 13.0966 sec.\n","반복 1010 || Loss: 0.0361 || 10iter: 13.1525 sec.\n","반복 1020 || Loss: 0.1159 || 10iter: 13.1240 sec.\n","반복 1030 || Loss: 0.0536 || 10iter: 13.1780 sec.\n","반복 1040 || Loss: 0.0496 || 10iter: 13.1460 sec.\n","반복 1050 || Loss: 0.0457 || 10iter: 13.2383 sec.\n","반복 1060 || Loss: 0.0563 || 10iter: 13.2048 sec.\n","반복 1070 || Loss: 0.0670 || 10iter: 13.1672 sec.\n","반복 1080 || Loss: 0.0614 || 10iter: 13.1734 sec.\n","반복 1090 || Loss: 0.0718 || 10iter: 13.1427 sec.\n","-------------\n","epoch 6 || Epoch_TRAIN_Loss:0.0623 ||Epoch_VAL_Loss:0.0000\n","timer:  261.4523 sec.\n","-------------\n","Epoch 7/30\n","-------------\n","(train)\n","반복 1100 || Loss: 0.0694 || 10iter: 1.7259 sec.\n","반복 1110 || Loss: 0.0482 || 10iter: 13.2123 sec.\n","반복 1120 || Loss: 0.0444 || 10iter: 13.1695 sec.\n","반복 1130 || Loss: 0.0689 || 10iter: 13.0896 sec.\n","반복 1140 || Loss: 0.0680 || 10iter: 13.1226 sec.\n","반복 1150 || Loss: 0.1521 || 10iter: 13.1954 sec.\n","반복 1160 || Loss: 0.0380 || 10iter: 13.1727 sec.\n","반복 1170 || Loss: 0.1002 || 10iter: 13.1639 sec.\n","반복 1180 || Loss: 0.0703 || 10iter: 13.1973 sec.\n","반복 1190 || Loss: 0.0466 || 10iter: 13.2236 sec.\n","반복 1200 || Loss: 0.0686 || 10iter: 13.2315 sec.\n","반복 1210 || Loss: 0.0380 || 10iter: 13.2208 sec.\n","반복 1220 || Loss: 0.0615 || 10iter: 13.1560 sec.\n","반복 1230 || Loss: 0.0336 || 10iter: 13.1546 sec.\n","반복 1240 || Loss: 0.0713 || 10iter: 13.1800 sec.\n","반복 1250 || Loss: 0.0538 || 10iter: 13.1965 sec.\n","반복 1260 || Loss: 0.0613 || 10iter: 13.0883 sec.\n","반복 1270 || Loss: 0.0370 || 10iter: 13.1554 sec.\n","반복 1280 || Loss: 0.0482 || 10iter: 13.1404 sec.\n","-------------\n","epoch 7 || Epoch_TRAIN_Loss:0.0589 ||Epoch_VAL_Loss:0.0000\n","timer:  261.9405 sec.\n","-------------\n","Epoch 8/30\n","-------------\n","(train)\n","반복 1290 || Loss: 0.0542 || 10iter: 11.7388 sec.\n","반복 1300 || Loss: 0.0269 || 10iter: 13.1321 sec.\n","반복 1310 || Loss: 0.0405 || 10iter: 13.1607 sec.\n","반복 1320 || Loss: 0.0630 || 10iter: 13.1539 sec.\n","반복 1330 || Loss: 0.0738 || 10iter: 13.1266 sec.\n","반복 1340 || Loss: 0.0542 || 10iter: 13.1535 sec.\n","반복 1350 || Loss: 0.0704 || 10iter: 13.1088 sec.\n","반복 1360 || Loss: 0.0503 || 10iter: 13.1943 sec.\n","반복 1370 || Loss: 0.0777 || 10iter: 13.1793 sec.\n","반복 1380 || Loss: 0.0496 || 10iter: 13.1078 sec.\n","반복 1390 || Loss: 0.0853 || 10iter: 13.1425 sec.\n","반복 1400 || Loss: 0.0586 || 10iter: 13.1993 sec.\n","반복 1410 || Loss: 0.0584 || 10iter: 13.1818 sec.\n","반복 1420 || Loss: 0.0628 || 10iter: 13.2178 sec.\n","반복 1430 || Loss: 0.1127 || 10iter: 13.1606 sec.\n","반복 1440 || Loss: 0.0477 || 10iter: 13.1995 sec.\n","반복 1450 || Loss: 0.0477 || 10iter: 13.1261 sec.\n","반복 1460 || Loss: 0.0837 || 10iter: 13.1742 sec.\n","-------------\n","epoch 8 || Epoch_TRAIN_Loss:0.0557 ||Epoch_VAL_Loss:0.0000\n","timer:  261.7087 sec.\n","-------------\n","Epoch 9/30\n","-------------\n","(train)\n","반복 1470 || Loss: 0.0366 || 10iter: 7.5041 sec.\n","반복 1480 || Loss: 0.0291 || 10iter: 13.1087 sec.\n","반복 1490 || Loss: 0.0711 || 10iter: 13.1565 sec.\n","반복 1500 || Loss: 0.0356 || 10iter: 13.1804 sec.\n","반복 1510 || Loss: 0.0707 || 10iter: 13.1334 sec.\n","반복 1520 || Loss: 0.0439 || 10iter: 13.1361 sec.\n","반복 1530 || Loss: 0.0488 || 10iter: 13.1452 sec.\n","반복 1540 || Loss: 0.0587 || 10iter: 13.2277 sec.\n","반복 1550 || Loss: 0.0279 || 10iter: 13.1349 sec.\n","반복 1560 || Loss: 0.0443 || 10iter: 13.2356 sec.\n","반복 1570 || Loss: 0.0530 || 10iter: 13.1394 sec.\n","반복 1580 || Loss: 0.0368 || 10iter: 13.1380 sec.\n","반복 1590 || Loss: 0.0920 || 10iter: 13.2071 sec.\n","반복 1600 || Loss: 0.0557 || 10iter: 13.2200 sec.\n","반복 1610 || Loss: 0.0368 || 10iter: 13.1349 sec.\n","반복 1620 || Loss: 0.0386 || 10iter: 13.1821 sec.\n","반복 1630 || Loss: 0.0452 || 10iter: 13.1626 sec.\n","반복 1640 || Loss: 0.0357 || 10iter: 13.1620 sec.\n","-------------\n","epoch 9 || Epoch_TRAIN_Loss:0.0523 ||Epoch_VAL_Loss:0.0000\n","timer:  261.8775 sec.\n","-------------\n","Epoch 10/30\n","-------------\n","(train)\n","반복 1650 || Loss: 0.0381 || 10iter: 3.1525 sec.\n","반복 1660 || Loss: 0.0396 || 10iter: 13.1556 sec.\n","반복 1670 || Loss: 0.0490 || 10iter: 13.1299 sec.\n","반복 1680 || Loss: 0.0367 || 10iter: 13.1830 sec.\n","반복 1690 || Loss: 0.0500 || 10iter: 13.1911 sec.\n","반복 1700 || Loss: 0.0505 || 10iter: 13.1861 sec.\n","반복 1710 || Loss: 0.0613 || 10iter: 13.0990 sec.\n","반복 1720 || Loss: 0.0599 || 10iter: 13.1185 sec.\n","반복 1730 || Loss: 0.0429 || 10iter: 13.1747 sec.\n","반복 1740 || Loss: 0.0511 || 10iter: 13.1269 sec.\n","반복 1750 || Loss: 0.0384 || 10iter: 13.1809 sec.\n","반복 1760 || Loss: 0.0458 || 10iter: 13.1582 sec.\n","반복 1770 || Loss: 0.0561 || 10iter: 13.1713 sec.\n","반복 1780 || Loss: 0.0397 || 10iter: 13.2181 sec.\n","반복 1790 || Loss: 0.0467 || 10iter: 13.1476 sec.\n","반복 1800 || Loss: 0.0325 || 10iter: 13.2127 sec.\n","반복 1810 || Loss: 0.0622 || 10iter: 13.1591 sec.\n","반복 1820 || Loss: 0.0758 || 10iter: 13.1265 sec.\n","반복 1830 || Loss: 0.0734 || 10iter: 13.1869 sec.\n","-------------\n","(val)\n","-------------\n","epoch 10 || Epoch_TRAIN_Loss:0.0509 ||Epoch_VAL_Loss:0.0761\n","timer:  347.7158 sec.\n","-------------\n","Epoch 11/30\n","-------------\n","(train)\n","반복 1840 || Loss: 0.0550 || 10iter: 13.1417 sec.\n","반복 1850 || Loss: 0.0442 || 10iter: 13.1973 sec.\n","반복 1860 || Loss: 0.0400 || 10iter: 13.1511 sec.\n","반복 1870 || Loss: 0.0305 || 10iter: 13.1946 sec.\n","반복 1880 || Loss: 0.0734 || 10iter: 13.1150 sec.\n","반복 1890 || Loss: 0.0287 || 10iter: 13.1229 sec.\n","반복 1900 || Loss: 0.0364 || 10iter: 13.1903 sec.\n","반복 1910 || Loss: 0.0492 || 10iter: 13.1989 sec.\n","반복 1920 || Loss: 0.0396 || 10iter: 13.1750 sec.\n","반복 1930 || Loss: 0.0288 || 10iter: 13.2164 sec.\n","반복 1940 || Loss: 0.0611 || 10iter: 13.1767 sec.\n","반복 1950 || Loss: 0.0604 || 10iter: 13.1957 sec.\n","반복 1960 || Loss: 0.0244 || 10iter: 13.2145 sec.\n","반복 1970 || Loss: 0.0430 || 10iter: 13.1645 sec.\n","반복 1980 || Loss: 0.0470 || 10iter: 13.1537 sec.\n","반복 1990 || Loss: 0.0386 || 10iter: 13.1581 sec.\n","반복 2000 || Loss: 0.0281 || 10iter: 13.1336 sec.\n","반복 2010 || Loss: 0.0495 || 10iter: 13.1415 sec.\n","-------------\n","epoch 11 || Epoch_TRAIN_Loss:0.0487 ||Epoch_VAL_Loss:0.0000\n","timer:  261.8923 sec.\n","-------------\n","Epoch 12/30\n","-------------\n","(train)\n","반복 2020 || Loss: 0.0458 || 10iter: 8.8907 sec.\n","반복 2030 || Loss: 0.0409 || 10iter: 13.1978 sec.\n","반복 2040 || Loss: 0.0263 || 10iter: 13.0855 sec.\n","반복 2050 || Loss: 0.0443 || 10iter: 13.1590 sec.\n","반복 2060 || Loss: 0.0214 || 10iter: 13.1200 sec.\n","반복 2070 || Loss: 0.0778 || 10iter: 13.1734 sec.\n","반복 2080 || Loss: 0.0502 || 10iter: 13.1251 sec.\n","반복 2090 || Loss: 0.0394 || 10iter: 13.1105 sec.\n","반복 2100 || Loss: 0.0328 || 10iter: 13.1024 sec.\n","반복 2110 || Loss: 0.0588 || 10iter: 13.1643 sec.\n","반복 2120 || Loss: 0.0305 || 10iter: 13.1415 sec.\n","반복 2130 || Loss: 0.0304 || 10iter: 13.1437 sec.\n","반복 2140 || Loss: 0.0453 || 10iter: 13.1378 sec.\n","반복 2150 || Loss: 0.0374 || 10iter: 13.1253 sec.\n","반복 2160 || Loss: 0.0532 || 10iter: 13.1712 sec.\n","반복 2170 || Loss: 0.0659 || 10iter: 13.1992 sec.\n","반복 2180 || Loss: 0.0518 || 10iter: 13.1534 sec.\n","반복 2190 || Loss: 0.0419 || 10iter: 13.1792 sec.\n","-------------\n","epoch 12 || Epoch_TRAIN_Loss:0.0481 ||Epoch_VAL_Loss:0.0000\n","timer:  261.5762 sec.\n","-------------\n","Epoch 13/30\n","-------------\n","(train)\n","반복 2200 || Loss: 0.0644 || 10iter: 4.5868 sec.\n","반복 2210 || Loss: 0.0424 || 10iter: 13.1545 sec.\n","반복 2220 || Loss: 0.0351 || 10iter: 13.1485 sec.\n","반복 2230 || Loss: 0.0576 || 10iter: 13.1633 sec.\n","반복 2240 || Loss: 0.0530 || 10iter: 13.1372 sec.\n","반복 2250 || Loss: 0.0411 || 10iter: 13.1162 sec.\n","반복 2260 || Loss: 0.0475 || 10iter: 13.1902 sec.\n","반복 2270 || Loss: 0.0478 || 10iter: 13.1668 sec.\n","반복 2280 || Loss: 0.0486 || 10iter: 13.1459 sec.\n","반복 2290 || Loss: 0.0313 || 10iter: 13.1682 sec.\n","반복 2300 || Loss: 0.0567 || 10iter: 13.1902 sec.\n","반복 2310 || Loss: 0.0390 || 10iter: 13.1408 sec.\n","반복 2320 || Loss: 0.0438 || 10iter: 13.2094 sec.\n","반복 2330 || Loss: 0.0281 || 10iter: 13.1467 sec.\n","반복 2340 || Loss: 0.0323 || 10iter: 13.1704 sec.\n","반복 2350 || Loss: 0.0556 || 10iter: 13.2020 sec.\n","반복 2360 || Loss: 0.0322 || 10iter: 13.2150 sec.\n","반복 2370 || Loss: 0.0542 || 10iter: 13.1354 sec.\n","-------------\n","epoch 13 || Epoch_TRAIN_Loss:0.0469 ||Epoch_VAL_Loss:0.0000\n","timer:  261.7965 sec.\n","-------------\n","Epoch 14/30\n","-------------\n","(train)\n","반복 2380 || Loss: 0.0294 || 10iter: 0.2757 sec.\n","반복 2390 || Loss: 0.0535 || 10iter: 13.1768 sec.\n","반복 2400 || Loss: 0.0393 || 10iter: 13.1505 sec.\n","반복 2410 || Loss: 0.0394 || 10iter: 13.2325 sec.\n","반복 2420 || Loss: 0.0437 || 10iter: 13.2014 sec.\n","반복 2430 || Loss: 0.0406 || 10iter: 13.1544 sec.\n","반복 2440 || Loss: 0.0316 || 10iter: 13.1433 sec.\n","반복 2450 || Loss: 0.0589 || 10iter: 13.1701 sec.\n","반복 2460 || Loss: 0.0451 || 10iter: 13.1802 sec.\n","반복 2470 || Loss: 0.0346 || 10iter: 13.1132 sec.\n","반복 2480 || Loss: 0.0291 || 10iter: 13.2042 sec.\n","반복 2490 || Loss: 0.0586 || 10iter: 13.1635 sec.\n","반복 2500 || Loss: 0.0393 || 10iter: 13.1733 sec.\n","반복 2510 || Loss: 0.0401 || 10iter: 13.1703 sec.\n","반복 2520 || Loss: 0.0575 || 10iter: 13.1341 sec.\n","반복 2530 || Loss: 0.0710 || 10iter: 13.1529 sec.\n","반복 2540 || Loss: 0.0413 || 10iter: 13.1957 sec.\n","반복 2550 || Loss: 0.0542 || 10iter: 13.2054 sec.\n","반복 2560 || Loss: 0.0343 || 10iter: 13.1838 sec.\n","-------------\n","epoch 14 || Epoch_TRAIN_Loss:0.0449 ||Epoch_VAL_Loss:0.0000\n","timer:  261.9495 sec.\n","-------------\n","Epoch 15/30\n","-------------\n","(train)\n","반복 2570 || Loss: 0.0421 || 10iter: 10.2105 sec.\n","반복 2580 || Loss: 0.0412 || 10iter: 13.1815 sec.\n","반복 2590 || Loss: 0.0382 || 10iter: 13.1543 sec.\n","반복 2600 || Loss: 0.0569 || 10iter: 13.1646 sec.\n","반복 2610 || Loss: 0.0320 || 10iter: 13.1437 sec.\n","반복 2620 || Loss: 0.0352 || 10iter: 13.1876 sec.\n","반복 2630 || Loss: 0.0286 || 10iter: 13.1227 sec.\n","반복 2640 || Loss: 0.0717 || 10iter: 13.1841 sec.\n","반복 2650 || Loss: 0.0425 || 10iter: 13.1893 sec.\n","반복 2660 || Loss: 0.0327 || 10iter: 13.2087 sec.\n","반복 2670 || Loss: 0.0385 || 10iter: 13.1554 sec.\n","반복 2680 || Loss: 0.0519 || 10iter: 13.1711 sec.\n","반복 2690 || Loss: 0.0441 || 10iter: 13.1494 sec.\n","반복 2700 || Loss: 0.0281 || 10iter: 13.1718 sec.\n","반복 2710 || Loss: 0.0446 || 10iter: 13.1515 sec.\n","반복 2720 || Loss: 0.0509 || 10iter: 13.1833 sec.\n","반복 2730 || Loss: 0.0313 || 10iter: 13.1241 sec.\n","반복 2740 || Loss: 0.0298 || 10iter: 13.1801 sec.\n","-------------\n","(val)\n","-------------\n","epoch 15 || Epoch_TRAIN_Loss:0.0459 ||Epoch_VAL_Loss:0.0717\n","timer:  347.5492 sec.\n","-------------\n","Epoch 16/30\n","-------------\n","(train)\n","반복 2750 || Loss: 0.0493 || 10iter: 6.0162 sec.\n","반복 2760 || Loss: 0.0394 || 10iter: 13.1585 sec.\n","반복 2770 || Loss: 0.0543 || 10iter: 13.1580 sec.\n","반복 2780 || Loss: 0.0682 || 10iter: 13.1504 sec.\n","반복 2790 || Loss: 0.0390 || 10iter: 13.1586 sec.\n","반복 2800 || Loss: 0.0568 || 10iter: 13.1292 sec.\n","반복 2810 || Loss: 0.0768 || 10iter: 13.1327 sec.\n","반복 2820 || Loss: 0.0456 || 10iter: 13.1256 sec.\n","반복 2830 || Loss: 0.0569 || 10iter: 13.1382 sec.\n","반복 2840 || Loss: 0.0350 || 10iter: 13.0929 sec.\n","반복 2850 || Loss: 0.0383 || 10iter: 13.1516 sec.\n","반복 2860 || Loss: 0.0665 || 10iter: 13.1679 sec.\n","반복 2870 || Loss: 0.0528 || 10iter: 13.1638 sec.\n","반복 2880 || Loss: 0.0289 || 10iter: 13.0860 sec.\n","반복 2890 || Loss: 0.0438 || 10iter: 13.1869 sec.\n","반복 2900 || Loss: 0.0591 || 10iter: 13.2100 sec.\n","반복 2910 || Loss: 0.0457 || 10iter: 13.1811 sec.\n","반복 2920 || Loss: 0.0597 || 10iter: 13.1158 sec.\n","-------------\n","epoch 16 || Epoch_TRAIN_Loss:0.0446 ||Epoch_VAL_Loss:0.0000\n","timer:  261.5493 sec.\n","-------------\n","Epoch 17/30\n","-------------\n","(train)\n","반복 2930 || Loss: 0.0464 || 10iter: 1.7165 sec.\n","반복 2940 || Loss: 0.0409 || 10iter: 13.1455 sec.\n","반복 2950 || Loss: 0.0395 || 10iter: 13.1557 sec.\n","반복 2960 || Loss: 0.0429 || 10iter: 13.1551 sec.\n","반복 2970 || Loss: 0.0485 || 10iter: 13.1853 sec.\n","반복 2980 || Loss: 0.0638 || 10iter: 13.1832 sec.\n","반복 2990 || Loss: 0.0307 || 10iter: 13.1924 sec.\n","반복 3000 || Loss: 0.0622 || 10iter: 13.2141 sec.\n","반복 3010 || Loss: 0.0296 || 10iter: 13.1193 sec.\n","반복 3020 || Loss: 0.0280 || 10iter: 13.1546 sec.\n","반복 3030 || Loss: 0.0352 || 10iter: 13.0932 sec.\n","반복 3040 || Loss: 0.0265 || 10iter: 13.1495 sec.\n","반복 3050 || Loss: 0.0647 || 10iter: 13.1622 sec.\n","반복 3060 || Loss: 0.0498 || 10iter: 13.2005 sec.\n","반복 3070 || Loss: 0.0347 || 10iter: 13.1340 sec.\n","반복 3080 || Loss: 0.0684 || 10iter: 13.1847 sec.\n","반복 3090 || Loss: 0.0545 || 10iter: 13.1709 sec.\n","반복 3100 || Loss: 0.0263 || 10iter: 13.1980 sec.\n","반복 3110 || Loss: 0.0369 || 10iter: 13.1277 sec.\n","-------------\n","epoch 17 || Epoch_TRAIN_Loss:0.0444 ||Epoch_VAL_Loss:0.0000\n","timer:  261.8162 sec.\n","-------------\n","Epoch 18/30\n","-------------\n","(train)\n","반복 3120 || Loss: 0.0371 || 10iter: 11.7359 sec.\n","반복 3130 || Loss: 0.0543 || 10iter: 13.1622 sec.\n","반복 3140 || Loss: 0.0627 || 10iter: 13.2375 sec.\n","반복 3150 || Loss: 0.0486 || 10iter: 13.2149 sec.\n","반복 3160 || Loss: 0.0197 || 10iter: 13.1870 sec.\n","반복 3170 || Loss: 0.0726 || 10iter: 13.1885 sec.\n","반복 3180 || Loss: 0.0350 || 10iter: 13.1366 sec.\n","반복 3190 || Loss: 0.0460 || 10iter: 13.1460 sec.\n","반복 3200 || Loss: 0.0509 || 10iter: 13.1641 sec.\n","반복 3210 || Loss: 0.0267 || 10iter: 13.1951 sec.\n","반복 3220 || Loss: 0.0542 || 10iter: 13.1157 sec.\n","반복 3230 || Loss: 0.0613 || 10iter: 13.1211 sec.\n","반복 3240 || Loss: 0.0466 || 10iter: 13.1533 sec.\n","반복 3250 || Loss: 0.0282 || 10iter: 13.1648 sec.\n","반복 3260 || Loss: 0.0415 || 10iter: 13.1735 sec.\n","반복 3270 || Loss: 0.0726 || 10iter: 13.1374 sec.\n","반복 3280 || Loss: 0.0519 || 10iter: 13.1174 sec.\n","반복 3290 || Loss: 0.0621 || 10iter: 13.1942 sec.\n","-------------\n","epoch 18 || Epoch_TRAIN_Loss:0.0444 ||Epoch_VAL_Loss:0.0000\n","timer:  261.8285 sec.\n","-------------\n","Epoch 19/30\n","-------------\n","(train)\n","반복 3300 || Loss: 0.0344 || 10iter: 7.4315 sec.\n","반복 3310 || Loss: 0.0238 || 10iter: 13.1869 sec.\n","반복 3320 || Loss: 0.0408 || 10iter: 13.1182 sec.\n","반복 3330 || Loss: 0.0493 || 10iter: 13.1666 sec.\n","반복 3340 || Loss: 0.0509 || 10iter: 13.1550 sec.\n","반복 3350 || Loss: 0.0329 || 10iter: 13.1105 sec.\n","반복 3360 || Loss: 0.0767 || 10iter: 13.1560 sec.\n","반복 3370 || Loss: 0.0229 || 10iter: 13.1922 sec.\n","반복 3380 || Loss: 0.0318 || 10iter: 13.1766 sec.\n","반복 3390 || Loss: 0.0429 || 10iter: 13.1467 sec.\n","반복 3400 || Loss: 0.0424 || 10iter: 13.1761 sec.\n","반복 3410 || Loss: 0.0345 || 10iter: 13.1125 sec.\n","반복 3420 || Loss: 0.0502 || 10iter: 13.2199 sec.\n","반복 3430 || Loss: 0.0319 || 10iter: 13.1329 sec.\n","반복 3440 || Loss: 0.0310 || 10iter: 13.1540 sec.\n","반복 3450 || Loss: 0.0424 || 10iter: 13.2176 sec.\n","반복 3460 || Loss: 0.0327 || 10iter: 13.1966 sec.\n","반복 3470 || Loss: 0.0336 || 10iter: 13.1320 sec.\n","-------------\n","epoch 19 || Epoch_TRAIN_Loss:0.0430 ||Epoch_VAL_Loss:0.0000\n","timer:  261.7721 sec.\n","-------------\n","Epoch 20/30\n","-------------\n","(train)\n","반복 3480 || Loss: 0.0340 || 10iter: 3.1484 sec.\n","반복 3490 || Loss: 0.0342 || 10iter: 13.0867 sec.\n","반복 3500 || Loss: 0.0715 || 10iter: 13.1303 sec.\n","반복 3510 || Loss: 0.0365 || 10iter: 13.1819 sec.\n","반복 3520 || Loss: 0.0378 || 10iter: 13.2423 sec.\n","반복 3530 || Loss: 0.0365 || 10iter: 13.1708 sec.\n","반복 3540 || Loss: 0.0456 || 10iter: 13.2526 sec.\n","반복 3550 || Loss: 0.0382 || 10iter: 13.1949 sec.\n","반복 3560 || Loss: 0.0283 || 10iter: 13.1842 sec.\n","반복 3570 || Loss: 0.0632 || 10iter: 13.1458 sec.\n","반복 3580 || Loss: 0.0222 || 10iter: 13.1678 sec.\n","반복 3590 || Loss: 0.0391 || 10iter: 13.2516 sec.\n","반복 3600 || Loss: 0.0319 || 10iter: 13.2026 sec.\n","반복 3610 || Loss: 0.0579 || 10iter: 13.1514 sec.\n","반복 3620 || Loss: 0.0376 || 10iter: 13.1851 sec.\n","반복 3630 || Loss: 0.0288 || 10iter: 13.2076 sec.\n","반복 3640 || Loss: 0.0440 || 10iter: 13.2713 sec.\n","반복 3650 || Loss: 0.0226 || 10iter: 13.2011 sec.\n","반복 3660 || Loss: 0.0481 || 10iter: 13.1885 sec.\n","-------------\n","(val)\n","-------------\n","epoch 20 || Epoch_TRAIN_Loss:0.0427 ||Epoch_VAL_Loss:0.0705\n","timer:  348.3742 sec.\n","-------------\n","Epoch 21/30\n","-------------\n","(train)\n","반복 3670 || Loss: 0.0381 || 10iter: 13.2276 sec.\n","반복 3680 || Loss: 0.0444 || 10iter: 13.1744 sec.\n","반복 3690 || Loss: 0.0333 || 10iter: 13.1324 sec.\n","반복 3700 || Loss: 0.0296 || 10iter: 13.1583 sec.\n","반복 3710 || Loss: 0.0230 || 10iter: 13.2126 sec.\n","반복 3720 || Loss: 0.0709 || 10iter: 13.1817 sec.\n","반복 3730 || Loss: 0.1010 || 10iter: 13.2185 sec.\n","반복 3740 || Loss: 0.0439 || 10iter: 13.1598 sec.\n","반복 3750 || Loss: 0.0269 || 10iter: 13.1460 sec.\n","반복 3760 || Loss: 0.0480 || 10iter: 13.1602 sec.\n","반복 3770 || Loss: 0.0325 || 10iter: 13.1353 sec.\n","반복 3780 || Loss: 0.0739 || 10iter: 13.2041 sec.\n","반복 3790 || Loss: 0.0539 || 10iter: 13.1822 sec.\n","반복 3800 || Loss: 0.0360 || 10iter: 13.1760 sec.\n","반복 3810 || Loss: 0.0505 || 10iter: 13.1932 sec.\n","반복 3820 || Loss: 0.0539 || 10iter: 13.1632 sec.\n","반복 3830 || Loss: 0.0380 || 10iter: 13.1681 sec.\n","반복 3840 || Loss: 0.0323 || 10iter: 13.1355 sec.\n","-------------\n","epoch 21 || Epoch_TRAIN_Loss:0.0424 ||Epoch_VAL_Loss:0.0000\n","timer:  261.9842 sec.\n","-------------\n","Epoch 22/30\n","-------------\n","(train)\n","반복 3850 || Loss: 0.0296 || 10iter: 8.8857 sec.\n","반복 3860 || Loss: 0.0789 || 10iter: 13.2263 sec.\n","반복 3870 || Loss: 0.0428 || 10iter: 13.2008 sec.\n","반복 3880 || Loss: 0.0232 || 10iter: 13.2140 sec.\n","반복 3890 || Loss: 0.0287 || 10iter: 13.1594 sec.\n","반복 3900 || Loss: 0.0411 || 10iter: 13.1989 sec.\n","반복 3910 || Loss: 0.0288 || 10iter: 13.1581 sec.\n","반복 3920 || Loss: 0.0247 || 10iter: 13.1966 sec.\n","반복 3930 || Loss: 0.0638 || 10iter: 13.1971 sec.\n","반복 3940 || Loss: 0.0330 || 10iter: 13.2166 sec.\n","반복 3950 || Loss: 0.0238 || 10iter: 13.1877 sec.\n","반복 3960 || Loss: 0.0591 || 10iter: 13.2073 sec.\n","반복 3970 || Loss: 0.0487 || 10iter: 13.1989 sec.\n","반복 3980 || Loss: 0.0219 || 10iter: 13.1634 sec.\n","반복 3990 || Loss: 0.0598 || 10iter: 13.1569 sec.\n","반복 4000 || Loss: 0.0496 || 10iter: 13.1909 sec.\n","반복 4010 || Loss: 0.0533 || 10iter: 13.1987 sec.\n","반복 4020 || Loss: 0.0355 || 10iter: 13.1760 sec.\n","-------------\n","epoch 22 || Epoch_TRAIN_Loss:0.0418 ||Epoch_VAL_Loss:0.0000\n","timer:  262.2147 sec.\n","-------------\n","Epoch 23/30\n","-------------\n","(train)\n","반복 4030 || Loss: 0.0757 || 10iter: 4.5862 sec.\n","반복 4040 || Loss: 0.0551 || 10iter: 13.1722 sec.\n","반복 4050 || Loss: 0.0454 || 10iter: 13.1338 sec.\n","반복 4060 || Loss: 0.0379 || 10iter: 13.1800 sec.\n","반복 4070 || Loss: 0.0327 || 10iter: 13.1481 sec.\n","반복 4080 || Loss: 0.0283 || 10iter: 13.2240 sec.\n","반복 4090 || Loss: 0.0495 || 10iter: 13.1856 sec.\n","반복 4100 || Loss: 0.0326 || 10iter: 13.1758 sec.\n","반복 4110 || Loss: 0.0436 || 10iter: 13.0764 sec.\n","반복 4120 || Loss: 0.0505 || 10iter: 13.1335 sec.\n","반복 4130 || Loss: 0.0407 || 10iter: 13.1383 sec.\n","반복 4140 || Loss: 0.0356 || 10iter: 13.1756 sec.\n","반복 4150 || Loss: 0.0593 || 10iter: 13.2043 sec.\n","반복 4160 || Loss: 0.0279 || 10iter: 13.1287 sec.\n","반복 4170 || Loss: 0.0754 || 10iter: 13.1576 sec.\n","반복 4180 || Loss: 0.0680 || 10iter: 13.1658 sec.\n","반복 4190 || Loss: 0.0535 || 10iter: 13.1619 sec.\n","반복 4200 || Loss: 0.0866 || 10iter: 13.1811 sec.\n","-------------\n","epoch 23 || Epoch_TRAIN_Loss:0.0432 ||Epoch_VAL_Loss:0.0000\n","timer:  261.8082 sec.\n","-------------\n","Epoch 24/30\n","-------------\n","(train)\n","반복 4210 || Loss: 0.0477 || 10iter: 0.2965 sec.\n","반복 4220 || Loss: 0.0557 || 10iter: 13.2270 sec.\n","반복 4230 || Loss: 0.0287 || 10iter: 13.1407 sec.\n","반복 4240 || Loss: 0.0522 || 10iter: 13.1613 sec.\n","반복 4250 || Loss: 0.0318 || 10iter: 13.1750 sec.\n","반복 4260 || Loss: 0.0492 || 10iter: 13.1896 sec.\n","반복 4270 || Loss: 0.0617 || 10iter: 13.2052 sec.\n","반복 4280 || Loss: 0.0353 || 10iter: 13.1824 sec.\n","반복 4290 || Loss: 0.0380 || 10iter: 13.1654 sec.\n","반복 4300 || Loss: 0.0378 || 10iter: 13.2150 sec.\n","반복 4310 || Loss: 0.0345 || 10iter: 13.1679 sec.\n","반복 4320 || Loss: 0.0264 || 10iter: 13.1473 sec.\n","반복 4330 || Loss: 0.0249 || 10iter: 13.1691 sec.\n","반복 4340 || Loss: 0.0278 || 10iter: 13.1350 sec.\n","반복 4350 || Loss: 0.0257 || 10iter: 13.1563 sec.\n","반복 4360 || Loss: 0.0416 || 10iter: 13.1663 sec.\n","반복 4370 || Loss: 0.0489 || 10iter: 13.1518 sec.\n","반복 4380 || Loss: 0.0614 || 10iter: 13.1692 sec.\n","반복 4390 || Loss: 0.0622 || 10iter: 13.1672 sec.\n","-------------\n","epoch 24 || Epoch_TRAIN_Loss:0.0412 ||Epoch_VAL_Loss:0.0000\n","timer:  261.9887 sec.\n","-------------\n","Epoch 25/30\n","-------------\n","(train)\n","반복 4400 || Loss: 0.0484 || 10iter: 10.3168 sec.\n","반복 4410 || Loss: 0.0329 || 10iter: 13.1558 sec.\n","반복 4420 || Loss: 0.0377 || 10iter: 13.1537 sec.\n","반복 4430 || Loss: 0.0251 || 10iter: 13.1636 sec.\n","반복 4440 || Loss: 0.0633 || 10iter: 13.2597 sec.\n","반복 4450 || Loss: 0.0280 || 10iter: 13.1569 sec.\n","반복 4460 || Loss: 0.0892 || 10iter: 13.1904 sec.\n","반복 4470 || Loss: 0.0351 || 10iter: 13.1528 sec.\n","반복 4480 || Loss: 0.0399 || 10iter: 13.1253 sec.\n","반복 4490 || Loss: 0.0340 || 10iter: 13.1175 sec.\n","반복 4500 || Loss: 0.0252 || 10iter: 13.1912 sec.\n","반복 4510 || Loss: 0.0598 || 10iter: 13.1128 sec.\n","반복 4520 || Loss: 0.0341 || 10iter: 13.2007 sec.\n","반복 4530 || Loss: 0.0533 || 10iter: 13.1242 sec.\n","반복 4540 || Loss: 0.0391 || 10iter: 13.1918 sec.\n","반복 4550 || Loss: 0.0387 || 10iter: 13.1527 sec.\n","반복 4560 || Loss: 0.0519 || 10iter: 13.1725 sec.\n","반복 4570 || Loss: 0.0491 || 10iter: 13.1821 sec.\n","-------------\n","(val)\n","-------------\n","epoch 25 || Epoch_TRAIN_Loss:0.0418 ||Epoch_VAL_Loss:0.0703\n","timer:  347.5330 sec.\n","-------------\n","Epoch 26/30\n","-------------\n","(train)\n","반복 4580 || Loss: 0.0248 || 10iter: 5.9968 sec.\n","반복 4590 || Loss: 0.0485 || 10iter: 13.1612 sec.\n","반복 4600 || Loss: 0.0655 || 10iter: 13.1671 sec.\n","반복 4610 || Loss: 0.0486 || 10iter: 13.1689 sec.\n","반복 4620 || Loss: 0.0359 || 10iter: 13.2219 sec.\n","반복 4630 || Loss: 0.0371 || 10iter: 13.1248 sec.\n","반복 4640 || Loss: 0.0387 || 10iter: 13.1336 sec.\n","반복 4650 || Loss: 0.0263 || 10iter: 13.1923 sec.\n","반복 4660 || Loss: 0.0639 || 10iter: 13.1903 sec.\n","반복 4670 || Loss: 0.0328 || 10iter: 13.1505 sec.\n","반복 4680 || Loss: 0.0288 || 10iter: 13.2382 sec.\n","반복 4690 || Loss: 0.0401 || 10iter: 13.2051 sec.\n","반복 4700 || Loss: 0.0379 || 10iter: 13.1708 sec.\n","반복 4710 || Loss: 0.0668 || 10iter: 13.1649 sec.\n","반복 4720 || Loss: 0.0412 || 10iter: 13.2476 sec.\n","반복 4730 || Loss: 0.0321 || 10iter: 13.1610 sec.\n","반복 4740 || Loss: 0.0400 || 10iter: 13.1275 sec.\n","반복 4750 || Loss: 0.0270 || 10iter: 13.1839 sec.\n","-------------\n","epoch 26 || Epoch_TRAIN_Loss:0.0403 ||Epoch_VAL_Loss:0.0000\n","timer:  262.0111 sec.\n","-------------\n","Epoch 27/30\n","-------------\n","(train)\n","반복 4760 || Loss: 0.0425 || 10iter: 1.7300 sec.\n","반복 4770 || Loss: 0.0341 || 10iter: 13.2028 sec.\n","반복 4780 || Loss: 0.0252 || 10iter: 13.1579 sec.\n","반복 4790 || Loss: 0.0398 || 10iter: 13.1316 sec.\n","반복 4800 || Loss: 0.0389 || 10iter: 13.1911 sec.\n","반복 4810 || Loss: 0.0268 || 10iter: 13.2029 sec.\n","반복 4820 || Loss: 0.0382 || 10iter: 13.1536 sec.\n","반복 4830 || Loss: 0.0552 || 10iter: 13.1597 sec.\n","반복 4840 || Loss: 0.0320 || 10iter: 13.1916 sec.\n","반복 4850 || Loss: 0.0532 || 10iter: 13.1684 sec.\n","반복 4860 || Loss: 0.0378 || 10iter: 13.2073 sec.\n","반복 4870 || Loss: 0.0428 || 10iter: 13.1227 sec.\n","반복 4880 || Loss: 0.0267 || 10iter: 13.1072 sec.\n","반복 4890 || Loss: 0.0411 || 10iter: 13.1590 sec.\n","반복 4900 || Loss: 0.0561 || 10iter: 13.1691 sec.\n","반복 4910 || Loss: 0.0413 || 10iter: 13.1419 sec.\n","반복 4920 || Loss: 0.0248 || 10iter: 13.2205 sec.\n","반복 4930 || Loss: 0.0363 || 10iter: 13.1915 sec.\n","반복 4940 || Loss: 0.0397 || 10iter: 13.1901 sec.\n","-------------\n","epoch 27 || Epoch_TRAIN_Loss:0.0391 ||Epoch_VAL_Loss:0.0000\n","timer:  261.9074 sec.\n","-------------\n","Epoch 28/30\n","-------------\n","(train)\n","반복 4950 || Loss: 0.0469 || 10iter: 11.7376 sec.\n","반복 4960 || Loss: 0.0420 || 10iter: 13.1858 sec.\n","반복 4970 || Loss: 0.0649 || 10iter: 13.2030 sec.\n","반복 4980 || Loss: 0.0381 || 10iter: 13.1521 sec.\n","반복 4990 || Loss: 0.0639 || 10iter: 13.2502 sec.\n","반복 5000 || Loss: 0.0490 || 10iter: 13.1893 sec.\n","반복 5010 || Loss: 0.0337 || 10iter: 13.1646 sec.\n","반복 5020 || Loss: 0.0344 || 10iter: 13.1927 sec.\n","반복 5030 || Loss: 0.0509 || 10iter: 13.2011 sec.\n","반복 5040 || Loss: 0.0362 || 10iter: 13.1562 sec.\n","반복 5050 || Loss: 0.0303 || 10iter: 13.1571 sec.\n","반복 5060 || Loss: 0.0194 || 10iter: 13.2001 sec.\n","반복 5070 || Loss: 0.0326 || 10iter: 13.1582 sec.\n","반복 5080 || Loss: 0.0377 || 10iter: 13.1840 sec.\n","반복 5090 || Loss: 0.0353 || 10iter: 13.1569 sec.\n","반복 5100 || Loss: 0.0629 || 10iter: 13.2360 sec.\n","반복 5110 || Loss: 0.0339 || 10iter: 13.1801 sec.\n","반복 5120 || Loss: 0.0413 || 10iter: 13.2088 sec.\n","-------------\n","epoch 28 || Epoch_TRAIN_Loss:0.0403 ||Epoch_VAL_Loss:0.0000\n","timer:  262.2285 sec.\n","-------------\n","Epoch 29/30\n","-------------\n","(train)\n","반복 5130 || Loss: 0.0571 || 10iter: 7.4154 sec.\n","반복 5140 || Loss: 0.0396 || 10iter: 13.1890 sec.\n","반복 5150 || Loss: 0.0375 || 10iter: 13.1155 sec.\n","반복 5160 || Loss: 0.0485 || 10iter: 13.1897 sec.\n","반복 5170 || Loss: 0.0769 || 10iter: 13.2536 sec.\n","반복 5180 || Loss: 0.0789 || 10iter: 13.1799 sec.\n","반복 5190 || Loss: 0.0562 || 10iter: 13.1410 sec.\n","반복 5200 || Loss: 0.0421 || 10iter: 13.1909 sec.\n","반복 5210 || Loss: 0.0285 || 10iter: 13.2048 sec.\n","반복 5220 || Loss: 0.0278 || 10iter: 13.1902 sec.\n","반복 5230 || Loss: 0.0216 || 10iter: 13.1733 sec.\n","반복 5240 || Loss: 0.0500 || 10iter: 13.2334 sec.\n","반복 5250 || Loss: 0.0310 || 10iter: 13.2541 sec.\n","반복 5260 || Loss: 0.0335 || 10iter: 13.1456 sec.\n","반복 5270 || Loss: 0.0359 || 10iter: 13.1874 sec.\n","반복 5280 || Loss: 0.0338 || 10iter: 13.1261 sec.\n","반복 5290 || Loss: 0.0492 || 10iter: 13.1237 sec.\n","반복 5300 || Loss: 0.0362 || 10iter: 13.1774 sec.\n","-------------\n","epoch 29 || Epoch_TRAIN_Loss:0.0398 ||Epoch_VAL_Loss:0.0000\n","timer:  262.1035 sec.\n","-------------\n","Epoch 30/30\n","-------------\n","(train)\n","반복 5310 || Loss: 0.0284 || 10iter: 3.1511 sec.\n","반복 5320 || Loss: 0.0327 || 10iter: 13.1656 sec.\n","반복 5330 || Loss: 0.0305 || 10iter: 13.1815 sec.\n","반복 5340 || Loss: 0.0565 || 10iter: 13.1750 sec.\n","반복 5350 || Loss: 0.0362 || 10iter: 13.1812 sec.\n","반복 5360 || Loss: 0.0404 || 10iter: 13.1554 sec.\n","반복 5370 || Loss: 0.0466 || 10iter: 13.1698 sec.\n","반복 5380 || Loss: 0.0395 || 10iter: 13.2035 sec.\n","반복 5390 || Loss: 0.0297 || 10iter: 13.1804 sec.\n","반복 5400 || Loss: 0.0635 || 10iter: 13.1367 sec.\n","반복 5410 || Loss: 0.0265 || 10iter: 13.1707 sec.\n","반복 5420 || Loss: 0.0237 || 10iter: 13.1531 sec.\n","반복 5430 || Loss: 0.0548 || 10iter: 13.1749 sec.\n","반복 5440 || Loss: 0.0301 || 10iter: 13.1537 sec.\n","반복 5450 || Loss: 0.0345 || 10iter: 13.2272 sec.\n","반복 5460 || Loss: 0.0596 || 10iter: 13.1622 sec.\n","반복 5470 || Loss: 0.0267 || 10iter: 13.1937 sec.\n","반복 5480 || Loss: 0.0248 || 10iter: 13.1911 sec.\n","반복 5490 || Loss: 0.0383 || 10iter: 13.1561 sec.\n","-------------\n","(val)\n","-------------\n","epoch 30 || Epoch_TRAIN_Loss:0.0399 ||Epoch_VAL_Loss:0.0705\n","timer:  347.9115 sec.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"uDlbnHaYo1sj"},"execution_count":null,"outputs":[]}]}